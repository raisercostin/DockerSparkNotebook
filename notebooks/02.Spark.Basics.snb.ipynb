{
  "metadata" : {
    "id" : "553e4db3-6bd9-426d-b623-eced20174f2c",
    "name" : "Spark Basics",
    "user_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "sparkNotebook" : null,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null,
    "customVars" : null
  },
  "cells" : [
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "CDE37C05CD5848ACA579FA84FC23519D"
      },
      "cell_type" : "code",
      "source" : [
        "val a = 1"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "id" : "DDB90329FC284E9D80EF42FA267E6D00"
      },
      "cell_type" : "markdown",
      "source" : "### Spark Basics: DataFrames and DataSets"
    },
    {
      "metadata" : {
        "id" : "C7896999DA2C4E6E86B5345E98926B5A"
      },
      "cell_type" : "markdown",
      "source" : "There are three different kinds of data modeling primitives that you can use in a Spark application to keep track of transparently distributed collections:  \n* RDDs (low-level)  \n* DataFrames (conceptually inspired by PyData / Pandas, available in Scala and Python)\n* DataSets (compile-time type-safe DataFrames, available in Scala but not in Python)"
    },
    {
      "metadata" : {
        "id" : "BACB3A90518E448CB946BAE436E10503"
      },
      "cell_type" : "markdown",
      "source" : "To exemplify a use case for DataFrames and DataSets, the first thing we are going to do is to define some input data by hand."
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "76AF2B989D474655A69023363225E150"
      },
      "cell_type" : "code",
      "source" : [
        "val rawData: String =\n",
        "  \"a,1\\nb,7\\na,4\\nb,3\\na,8\\nb,2\\na,5\\nb,4\\na,7\\na,9\\nb,1\""
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "D0C59C699E7B4B7A9B56B623F5930011"
      },
      "cell_type" : "code",
      "source" : [
        "val dataAfterSplit = rawData.split(\"\\n\")"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "C5C011457C124BB288B0334806A5B9F2"
      },
      "cell_type" : "code",
      "source" : [
        "dataAfterSplit(0)"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "id" : "E80C66237AE9486E827F074F380C8506"
      },
      "cell_type" : "markdown",
      "source" : "You'll notice that each record consists of a text key and a numeric value separated by a comma.  \nNext, we define a case class to represent this type of data record."
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "BB0E4BBC5E4145599BA66214F1E975FD"
      },
      "cell_type" : "code",
      "source" : [
        "case class DataRecord(key: String, value: Int)"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "id" : "C55FD23B4B2D4912974B1D6D294C1525"
      },
      "cell_type" : "markdown",
      "source" : "We also define a function to parse our raw CSV records into type-safe **`DataRecord`**s."
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "0C9CB55FE51D46E382F3F03C98939860"
      },
      "cell_type" : "code",
      "source" : [
        "def parseIntoDataRecord(s: String): DataRecord = {\n",
        "  val afterSplit = s.split(\",\")\n",
        "  DataRecord( afterSplit(0), afterSplit(1).toInt )\n",
        "}"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "id" : "C51F7FA8211A44F18D28C319A5E44A84"
      },
      "cell_type" : "markdown",
      "source" : "Next, we create first a **`DataFrame`** and then a **`DataSet`** based on our data."
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "44F63CBA4F7642F98CFFF2C68FAB8716"
      },
      "cell_type" : "code",
      "source" : [
        "val myFirstDataFrame =\n",
        "  sparkSession.createDataFrame( dataAfterSplit.map( parseIntoDataRecord(_) ) )"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "44F63CBA4F7642F98CFFF2C68FAB8716"
      },
      "cell_type" : "code",
      "source" : [
        "val myFirstDataSet = myFirstDataFrame.as[DataRecord]"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "id" : "9EF577D7E3F94262B6520771F67CE8C3"
      },
      "cell_type" : "markdown",
      "source" : "The data has now been distributed across the Spark cluster, let's have a look at it."
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "D070F1331C404072857499E793EA6E0B"
      },
      "cell_type" : "code",
      "source" : [
        "myFirstDataSet.cache"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "EB124C0F0BD846B0978F9DFE81E6AA0D"
      },
      "cell_type" : "code",
      "source" : [
        "myFirstDataSet.count"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "id" : "FF6AECEA66C14C9481F1E755A6698349"
      },
      "cell_type" : "markdown",
      "source" : "We can now work with our data using Spark SQL, which is a SQL-2003 compliant language.  \nAll we have to do is attach a table name to our **`DataSet`**."
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "9D5730ABE0C343FF8E92007CC5DEC70E"
      },
      "cell_type" : "code",
      "source" : [
        "myFirstDataSet.createOrReplaceTempView(\"raw_data\")\n",
        "\n",
        "println(\"Done.\")"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "E0C1A3EF83A149439D4CEC6F13D5940A"
      },
      "cell_type" : "code",
      "source" : [
        "sparkSession.sql(\"\"\"SELECT key,\n",
        "                           SUM(value) AS sum_value\n",
        "                    FROM raw_data\n",
        "                    GROUP BY key\n",
        "                    ORDER BY key\"\"\")"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "E0C1A3EF83A149439D4CEC6F13D5940A"
      },
      "cell_type" : "code",
      "source" : [
        "val aggregatedDataFrame =\n",
        "  sparkSession.sql(\"\"\"SELECT key,\n",
        "                           SUM(value) AS sum_value\n",
        "                    FROM raw_data\n",
        "                    GROUP BY key\n",
        "                    ORDER BY key\"\"\")"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "A3457856EF3041B584A6F474AC75E970"
      },
      "cell_type" : "code",
      "source" : [
        "val filteredDataFrame =\n",
        "  sparkSession.sql(\"SELECT * FROM raw_data WHERE value < 5\")"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "97FCA97AA0344785BD8500FCF9F9B9BD"
      },
      "cell_type" : "code",
      "source" : [
        "filteredDataFrame"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "id" : "44CB275BCC0C4BDB891F69356D4923DE"
      },
      "cell_type" : "markdown",
      "source" : "#### User Defined Functions (UDFs)"
    },
    {
      "metadata" : {
        "id" : "CE224D287CB44EC5AA226EB33A70FC76"
      },
      "cell_type" : "markdown",
      "source" : "User-Defined Functions (UDFs) is a feature of Spark SQL to define new column-based functions.  \nUDFs are very effective because they can be materialized anywhere in the cluster, so the function can be at the same physical location as the data, this makes operating on data using UDFs very efficient."
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "2E55962365E84C02937197C91E897566"
      },
      "cell_type" : "code",
      "source" : [
        "// First we define a Scala function\n",
        "def upCaseSqBr(inputStr: String): String = s\"[${inputStr.toUpperCase}]\"\n",
        "\n",
        "println( upCaseSqBr(\"abc\") )"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "9D23502E9CD544C88CC9A8420E0D3CA4"
      },
      "cell_type" : "code",
      "source" : [
        "// Then we register its alias in Spark SQL\n",
        "sparkSession.udf.register[String, String]( \"upCaseSqBrUDF\", upCaseSqBr(_) )"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "41277640E8CA4E6F8DF7C06000AD1A11"
      },
      "cell_type" : "code",
      "source" : [
        "val dataFrameAfterUDF =\n",
        "  sparkSession.sql(\"\"\"SELECT upCaseSqBrUDF(key) AS up_case_sq_br,\n",
        "                             value\n",
        "                      FROM raw_data\"\"\")"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "47911BD5416143F2BC418916CAD6D80B"
      },
      "cell_type" : "code",
      "source" : [
        "dataFrameAfterUDF.cache"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "F9F5D1D5F15E4184856D7EF1B9A87FE0"
      },
      "cell_type" : "code",
      "source" : [
        "dataFrameAfterUDF.count"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "9F7877FCF5F34CAB8883D027667505E5"
      },
      "cell_type" : "code",
      "source" : [
        "dataFrameAfterUDF.createOrReplaceTempView(\"data_after_udf\")"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "0E6882AEF10545949CEC00B304D09251"
      },
      "cell_type" : "code",
      "source" : [
        "val stmt = \"\"\"\n",
        "  SELECT ROW_NUMBER() OVER(PARTITION BY up_case_sq_br\n",
        "                           ORDER BY value DESC) as rownum, *\n",
        "  FROM data_after_udf\"\"\""
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "0E6882AEF10545949CEC00B304D09251"
      },
      "cell_type" : "code",
      "source" : [
        "sparkSession.sql(stmt)"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "0E6882AEF10545949CEC00B304D09251"
      },
      "cell_type" : "code",
      "source" : [
        "sparkSession.sql(stmt)\n",
        "            .where(\"rownum = 1\")"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "id" : "BF7C72E54455478D987E38D18CCC1105"
      },
      "cell_type" : "markdown",
      "source" : "Going back to the topic of UDFs, a more realistic example of a UDF is one that generates a UUID."
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "370DD36C00154E968305433327279A25"
      },
      "cell_type" : "code",
      "source" : [
        "import java.util.UUID\n",
        "\n",
        "sparkSession.udf.register[String](\"uuid\", () => UUID.randomUUID.toString)"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "B3BBCB40DDCD492C86C8094E2DD38A2E"
      },
      "cell_type" : "code",
      "source" : [
        "val dataFrameWithUUID =\n",
        "  sparkSession.sql(\"SELECT uuid() AS uuid, * FROM raw_data\")"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "3F7B63EFD4E54AA98690DF01B4ED17CA"
      },
      "cell_type" : "code",
      "source" : [
        "dataFrameWithUUID.cache"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "id" : "E06C50A4A1E94C80836C3B0D4DFDD7A8"
      },
      "cell_type" : "markdown",
      "source" : "#### Joining Two DataSets"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "83AEF9156C054E90801B945831FFB105"
      },
      "cell_type" : "code",
      "source" : [
        ":sh head -5 /opt/SparkDatasets/geography/cities.csv"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "83AEF9156C054E90801B945831FFB105"
      },
      "cell_type" : "code",
      "source" : [
        ":sh cat /opt/SparkDatasets/geography/cities_header.csv"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "AA0136F2F233423BBFD827FE64264021"
      },
      "cell_type" : "code",
      "source" : [
        "case class City (city_id: Long,\n",
        "                 country_id: Long,\n",
        "                 city_name: String)"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "83AEF9156C054E90801B945831FFB105"
      },
      "cell_type" : "code",
      "source" : [
        ":sh head -5 /opt/SparkDatasets/geography/countries.csv"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "83AEF9156C054E90801B945831FFB105"
      },
      "cell_type" : "code",
      "source" : [
        ":sh cat /opt/SparkDatasets/geography/countries_header.csv"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "C7B476C18049456D8C58FB86E37DFE71"
      },
      "cell_type" : "code",
      "source" : [
        "case class Country (country_id: Long,\n",
        "                    continent_id: Long,\n",
        "                    country_name: String)"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "543C1675BF194E77868F48603246A550"
      },
      "cell_type" : "code",
      "source" : [
        "import org.apache.spark.sql.Encoders\n",
        "\n",
        "val citySchema = Encoders.product[City].schema\n",
        "val countrySchema = Encoders.product[Country].schema"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "5FAEB99967BC458388F384CA0F4E03F9"
      },
      "cell_type" : "code",
      "source" : [
        "val citiesDS =\n",
        "  sparkSession.read\n",
        "              .schema(citySchema)\n",
        "              .csv(\"/opt/SparkDatasets/geography/cities.csv\")\n",
        "              .as[City]\n",
        "\n",
        "citiesDS.cache\n",
        "\n",
        "citiesDS.count\n",
        "\n",
        "citiesDS.createOrReplaceTempView(\"cities\")"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "5FAEB99967BC458388F384CA0F4E03F9"
      },
      "cell_type" : "code",
      "source" : [
        "val countriesDS =\n",
        "  sparkSession.read\n",
        "              .schema(countrySchema)\n",
        "              .csv(\"/opt/SparkDatasets/geography/countries.csv\")\n",
        "              .as[Country]\n",
        "\n",
        "countriesDS.cache\n",
        "\n",
        "countriesDS.count\n",
        "\n",
        "countriesDS.createOrReplaceTempView(\"countries\")"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "9569C6104B80449BB20CCBEC98EF008F"
      },
      "cell_type" : "code",
      "source" : [
        "sparkSession.sql(\"\"\"SELECT countries.country_name,\n",
        "                           cities.city_name\n",
        "                    FROM cities INNER JOIN countries\n",
        "                    ON cities.country_id = countries.country_id\"\"\")"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "id" : "099A4F8C553846318A49786DC0B1C6A8"
      },
      "cell_type" : "markdown",
      "source" : "#### Ingesting well-behaved JSON"
    },
    {
      "metadata" : {
        "id" : "8F5F993172774D0384DD566761C24FBA"
      },
      "cell_type" : "markdown",
      "source" : "Given the population data below,  \nand the assumption that zip codes which are close to each other geographically are also close to each other numerically,  \nlet's compute the top 10 list of the most densely populated \"pockets\" of zip codes, along with the U.S. state they are in.  \n(We arbitrarily define a \"pocket\" as the geographical proximity indicated by the first 4 digits of the zip code.)"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "23C62C362D3B46728BD2984C6A46BB92"
      },
      "cell_type" : "code",
      "source" : [
        ":sh head -5 /opt/SparkDatasets/zipcodes/zips.json"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "630D2D678F0E4F8C9EBD9FCA1880112C"
      },
      "cell_type" : "code",
      "source" : [
        "sparkSession.read\n",
        "            .json(\"/opt/SparkDatasets/zipcodes/zips.json\")\n",
        "            .printSchema"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "73170BDF765D46F584B03F312F41168C"
      },
      "cell_type" : "code",
      "source" : [
        "import org.apache.spark.sql.Encoders\n",
        "\n",
        "// Define the case class\n",
        "case class PopulationData(city: String,\n",
        "                          loc: Array[Double],\n",
        "                          pop: Long,\n",
        "                          state: String,\n",
        "                          zip_code: String)\n",
        "\n",
        "// Define the schema\n",
        "val pDataSchema = Encoders.product[PopulationData].schema"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "B377C1F5538C41379C91F98E7C62388B"
      },
      "cell_type" : "code",
      "source" : [
        "// Ingest the data\n",
        "val pDataDS =\n",
        "  sparkSession.read\n",
        "              .schema(pDataSchema)\n",
        "              .json(\"/opt/SparkDatasets/zipcodes/zips.json\")\n",
        "              .as[PopulationData]\n",
        "\n",
        "pDataDS.cache\n",
        "\n",
        "// Define the temp view\n",
        "pDataDS.createOrReplaceTempView(\"population_data\")\n",
        "\n",
        "pDataDS.count"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "46EE3EF296DD4DD4B0DA4DDB6FB0112B"
      },
      "cell_type" : "code",
      "source" : [
        "// Define the UDF\n",
        "sparkSession.udf.register[String, String](\"first_four\", _.take(4) )\n",
        "\n",
        "// Write the SQL statement\n",
        "sparkSession.sql(\"\"\"SELECT first_four(zip_code) AS zc_first_four,\n",
        "                           state,\n",
        "                           SUM(pop) AS zone_population\n",
        "                    FROM population_data\n",
        "                    GROUP BY zc_first_four, state\n",
        "                    ORDER BY zone_population DESC\"\"\")"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "id" : "099A4F8C553846318A49786DC0B1C6A8"
      },
      "cell_type" : "markdown",
      "source" : "#### Ingesting real-world JSON"
    },
    {
      "metadata" : {
        "id" : "F5AF2423BA8345F3969DC8359ED60C00"
      },
      "cell_type" : "markdown",
      "source" : "The dataset below contains DC/OS specific download counts for various open-source big data packages.  \nOur goal is to extract a top 5 of the most downloaded packages.  \nWe'd like to see the package name, the month and the download count.  \nFirst of all, here is the human-friendly representation of the JSON records we are going to ingest:"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "A270BF7A1EF94B5FA225B43F2C9615DD"
      },
      "cell_type" : "code",
      "source" : [
        ":sh cat /opt/SparkDatasets/dcos/universe.json"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "id" : "0D526EA74B1B4A70831F3F34B74E5252"
      },
      "cell_type" : "markdown",
      "source" : "And here is the exact same data, but in newline-separated JSON format:"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "A270BF7A1EF94B5FA225B43F2C9615DD"
      },
      "cell_type" : "code",
      "source" : [
        ":sh cat /opt/SparkDatasets/dcos/universe.jsonline"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "id" : "996F046B0B5847418DF3F0CC54513A57"
      },
      "cell_type" : "markdown",
      "source" : "Our first instinct would be to do as we did before.  \nLet's see if that works."
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "FD01A516E78744EF9F70E1FB4B390C24"
      },
      "cell_type" : "code",
      "source" : [
        "sparkSession.read\n",
        "            .json(\"/opt/SparkDatasets/dcos/universe.jsonline\")\n",
        "            .printSchema"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "id" : "BD8F9A32FD434BCFB8A36EBD36EDC688"
      },
      "cell_type" : "markdown",
      "source" : "Attempting to use the same methodology as before leads to a dead end.  \nTherefore something better is needed.  \nLet's start by breaking down the first record of the dataset, and after that we'll generalize to the entire data set."
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "193CF5E8EBC240E4A14A38E7F9F2821B"
      },
      "cell_type" : "code",
      "source" : [
        "// First record only\n",
        "val alluxio =\n",
        "  sparkSession.read\n",
        "              .text(\"/opt/SparkDatasets/dcos/universe.jsonline\")\n",
        "              .first"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "D5A1AE1A36554321A562934137CADB44"
      },
      "cell_type" : "code",
      "source" : [
        "alluxio.mkString"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "id" : "E931D8E611064A72A53FCC97C1DCAB72"
      },
      "cell_type" : "markdown",
      "source" : "We parse it with `JSON4S`:"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "8219297497AA4C5E81DBFD3EC3D24274"
      },
      "cell_type" : "code",
      "source" : [
        "import org.json4s.native.JsonMethods.parse\n",
        "\n",
        "parse(alluxio.mkString)"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "id" : "AB38BD97823349C4815CCD7C974CB983"
      },
      "cell_type" : "markdown",
      "source" : "Since we only have to deal with one key-value pair, let's treat it as a tuple:"
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "61B37E3446F643BC9E3015C8FE01352F"
      },
      "cell_type" : "code",
      "source" : [
        "val (pkgName, pkgContents) = {\n",
        "  import org.json4s.DefaultFormats\n",
        "\n",
        "  implicit val formats = DefaultFormats\n",
        "\n",
        "  parse(alluxio.mkString).extractOpt[(String,Map[String,Any])]\n",
        "                         .getOrElse[(String,Map[String,Any])]( (\"\",Map()) )\n",
        "}"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "697D09CF230E43EE89E4384469372780"
      },
      "cell_type" : "code",
      "source" : [
        "case class PackageDownloadCount(packageName: String,\n",
        "                                month: String,\n",
        "                                downloadCount: Long)"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "presentation" : {
          "tabs_state" : "{\n  \"tab_id\": \"#tab1155433370-0\"\n}",
          "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
        },
        "id" : "5AB665EB24AE4065B816DD9CDB018DDA"
      },
      "cell_type" : "code",
      "source" : [
        "pkgContents(\"downloads\")\n",
        "  .asInstanceOf[Map[String,BigInt]]\n",
        "  .view\n",
        "  .map { case (month, downloadCount) =>\n",
        "           PackageDownloadCount(pkgName,\n",
        "                                month,\n",
        "                                downloadCount.longValue) }\n",
        "  .toSeq"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "id" : "8AE2F15EEEA34D72A50C930F33E4C78D"
      },
      "cell_type" : "markdown",
      "source" : "We define our parse function by simply copying and pasting the exploratory code we've written above."
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "91B1CB6AA09043FC87C9C1A459D424E5"
      },
      "cell_type" : "code",
      "source" : [
        "import org.apache.spark.sql.Row\n",
        "import org.json4s.native.JsonMethods.parse\n",
        "\n",
        "def parseOneJSONRecord(inputRow: Row): TraversableOnce[PackageDownloadCount] = {\n",
        "  import org.json4s.DefaultFormats\n",
        "\n",
        "  implicit val formats = DefaultFormats\n",
        "  \n",
        "  val (pkgName, pkgContents) =\n",
        "    parse(inputRow.mkString).extractOpt[(String,Map[String,Any])]\n",
        "                            .getOrElse[(String,Map[String,Any])]( (\"\",Map()) )\n",
        "  pkgContents(\"downloads\")\n",
        "    .asInstanceOf[Map[String,BigInt]]\n",
        "    .view\n",
        "    .map { case (month, downloadCount) =>\n",
        "             PackageDownloadCount(pkgName,\n",
        "                                  month,\n",
        "                                  downloadCount.longValue) }\n",
        "    .toSeq\n",
        "}"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "BBEDEBC7F40B4741826CD893281E67A6"
      },
      "cell_type" : "code",
      "source" : [
        "val downloadCountsDS =\n",
        "  sparkSession.read\n",
        "              .text(\"/opt/SparkDatasets/dcos/universe.jsonline\")\n",
        "              .flatMap( parseOneJSONRecord(_) )\n",
        "              .as[PackageDownloadCount]"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "03C3719BE95542B19A2D6EA37CA26AAF"
      },
      "cell_type" : "code",
      "source" : [
        "downloadCountsDS.cache"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "623401FC775648DC816F2734E749898D"
      },
      "cell_type" : "code",
      "source" : [
        "downloadCountsDS.count"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "983F44A8955242D4807BFA5FD6DBCC16"
      },
      "cell_type" : "code",
      "source" : [
        "downloadCountsDS.orderBy('downloadCount.desc).limit(5)"
      ],
      "outputs" : [ ]
    },
    {
      "metadata" : {
        "trusted" : true,
        "input_collapsed" : false,
        "collapsed" : true,
        "id" : "3AFBBFA9EA0D4DA188BCD280C4122E2E"
      },
      "cell_type" : "code",
      "source" : [
        ""
      ],
      "outputs" : [ ]
    }
  ],
  "nbformat" : 4
}